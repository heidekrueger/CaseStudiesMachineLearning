%!TEX program = xelatex
\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle, noframetitleoffset, block=fill]{m}
%\definecolor{TUMblue}{RGB}{55,55,255}
%\setbeamercolor{alerted text}{fg=TUMblue}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepackage{tikz}
\usepgfplotslibrary{dateplot}
\usepackage{caption}

\newlength\figureheight
\newlength\figurewidth
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\argmin}{argmin}

\title{Stochastic Optimization in Machine Learning}
\subtitle{Case Studies in Nonlinear Optimization}
\date{\today}
\author{F. Bauer \and S. Chambon \and R. Halbig \and S. Heidekrüger \and J. Heuke}
\institute{Technische Universität München}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.eps}}

\begin{document}

\maketitle

\plain{
  \begin{quote}
    We're not running out of data anytime soon. It's maybe the only resource that grows exponentially.
    \\
    \flushright{\alert{Andreas Weigend}}
  \end{quote}
  }


\begin{frame}
  \frametitle{Outline}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}

  \begin{frame}[t]\frametitle{Introduction: What is Machine Learning?}
	  	Implementation of autonomously learning software for:
        \begin{itemize}
        	\item Discovery of patterns and relationships in data
        	\item Prediction of future events
        \end{itemize}
        \vspace{5pt}
        \alert{Examples:}
        \begin{columns}
        	\begin{column}{.2\linewidth}
        		Higgs-Boson\\
        		\includegraphics[width = \linewidth]{CMS_Higgs-event.jpg}\\
        		\alert{Section 2}
        	\end{column}\hspace{-10pt}
        	\begin{column}{.2\linewidth}
        		Computed Tomography (CT)\\
        		\includegraphics[width =\linewidth]{plot_tomography_l1_reconstruction_001.png}\\
        		\alert{Section 3}
        	\end{column}\hspace{-10pt}
        	\begin{column}{.2\linewidth}
        		Electroence-phalography (EEG)\\
        		\includegraphics[width = \linewidth]{eeg_pic.jpg}\\
        		\alert{Section 4}
        	\end{column}\hspace{-10pt}
        	\begin{column}{.2\linewidth}
        		Image Denoising\\
        		\includegraphics[width = \linewidth]{lena_pic.jpg}\\
        		\alert{Section 5}
        	\end{column}
        \end{columns}
  \end{frame}

  \begin{frame}\frametitle{ML and Optimization I}
    \alert{Training} a Machine Learning model means finding optimal parameters $\omega$:

    $$ \omega^* = \argmin_{\omega} F(\omega, X, z)$$
    \pause
    \begin{itemize}
      \item \alert{$F$}: Loss function of chosen ML-model
      \item \alert{$X$}: The training data ($N:=\#$samples $\times$ $\#$features matrix)
      \item \alert{$z$}: Training labels (only in classification models; vector of size $N$)
      \pause
      \item The dimension $n$ of $\omega$ is model dependent, often $\#$features$+1$
    \end{itemize}   
  \end{frame}

  \begin{frame}\frametitle{ML and Optimization II}
    After we have found $\omega^*$, we can do \alert{Prediction} on new data points:

    $$ \hat {z_i} := h(\omega^*, x_i)$$
    \pause
    \begin{itemize}
      \item \alert{$x_i$}: new data point with \emph{unknown} label \alert{$z_i$}
      \item \alert{$h$}: hypothesis function of the ML model
    \end{itemize}   
  \end{frame}

  \begin{frame}
    \frametitle{Challenges in Machine Learning}
      \begin{itemize}
        \item Massive amounts of training data 
        \item Construction of very large models
        \item Handling high memory/computational demands
      \end{itemize}
      \vspace{36pt}
      \pause
    \centering \large{Ansatz: \alert{Stochastic Methods}}
  \end{frame}
  
  \begin{frame}{Stochastic Framework}
    $$ F(\omega) := \mathbb{E}\left[f(\omega, \xi)\right] \uncover<3>{= \frac{1}{N}\sum_{i=1}^N f(\omega, x_i, z_i)}$$
    \begin{itemize}
      \item<2-> \alert{$\xi$}: Random variable; takes the form of an input-output-pair $(x_i, z_i)$
      \item<3-> \alert{$f$}: Partial loss function corresponding to a single data point.
    \end{itemize}
  \end{frame}

  \begin{frame}{Stochastic Methods}
    \begin{columns}[T]
      \begin{column}{.5\textwidth}
        \centering \alert{Gradient Method}
        $$\min F(\omega) $$

        \uncover<2->{
        $$\omega^{(k+1)}:= \omega^{(k)}-\alpha_k \nabla F(\omega^{(k)})$$\\
        \phantom{zeile}
        }
      \end{column}\hfill
      \begin{column}{.5\textwidth}
        \centering \alert{Stochastic Gradient Descent}
        $$\min \mathbb E \left [f(\omega, \xi)\right]$$
        \uncover<3>{
          $$\omega^{(k+1)}:= \omega^{(k)}-\alpha_k \alert{\nabla \hat F(\omega^{(k)})} $$
          with
          $$\alert{\nabla \hat F(\omega^{(k)})} := \frac{1}{b}\sum_{i\in \mathcal S_k}f(\omega, x_i, z_i)$$
          where $\mathcal S_k \subset [N], \quad b:=|\mathcal S_k| \ll N$\\\alert{"Mini Batch"}
        }
      \end{column}
    \end{columns}
  \end{frame}

\section{SQN: A Stochastic Quasi-Newton Method}
 \plain{Classification\\
 	\vspace{10pt}
 	\alert{Did we just detect a Higgs-Boson?}
 	\vspace{15pt}\\
 	\includegraphics[width = 0.5\textwidth]{CMS_Higgs-event.jpg}}

  \begin{frame}
    \frametitle{Higgs-Boson classification problem}
      \begin{itemize}
        \item Data from Monte-Carlo simulations
        \item $X\in \mathbb R^{11.000.000 \times 29}$\\\emph{Lots} of samples, relatively small, dense feature set.
        \item Here, we use \emph{Logistic Regression} for classification.
      \end{itemize}
  \end{frame}

  \begin{frame}\frametitle{Stochastic Quasi-Newton Method (SQN)}
      \begin{itemize}
        \item \alert{Stochastically} use second-order information
        \item Based on BFGS-method.
        \pause
        \item Basic idea: $$ \omega^{(k+1)} = \omega^{(k)} - \alpha_k \alert{H_t} \nabla \hat F(\omega^{(k)})$$
        \pause
        \item $t$ running on slower time-scale than $k$. 
        \item $H_t$ update in $\mathcal O(n)$ time and constant memory, using several tricks
      \end{itemize}
  \end{frame}


  \begin{frame}
    \frametitle{Behavior}
      Pretty picures about the behaviour of SQN on HIGGS
      and comparison with traditional SGD
  \end{frame}

  \begin{frame}\frametitle{Results}
    \begin{itemize}
      \item Can be faster than SGD on appropriate Datasets
      \item Requires tedious, manual tuning of hyperparameters to be efficient!
    \end{itemize}
  
  \end{frame}

 \section{Proximal Method}
 \plain{Image Reconstruction\\
 	\vspace{10pt}
 	\alert{What did the original image look like?}
 	\vspace{15pt}\\
 	\includegraphics[width = 0.8\textwidth]{plot_tomography_l1_reconstruction_001.png}}
 
 
   \begin{frame}\frametitle{Proximal Method}
       \begin{flalign*}
       	\text{\alert{Problem}}&&
       	\min_x &\;F(x) := \underbrace{f(x)}_{smooth} \quad + \underbrace{h(x)}_{non-smooth}&
       \end{flalign*}
       \pause
       \begin{flalign*}
       	\text{\alert{Proximity Operator}}&&\prox_f(v) = &\underset{x}{\argmin} \; \bigl( f(x) + \frac{1}{2} \lVert x - v \rVert^2_2 \bigr)&
       \end{flalign*}
			\centering\includegraphics[width = 0.5\textwidth]{prox_boyd.jpg}
   \end{frame}
   
   \begin{frame}{Proximal Method}
   	\alert{Traditional Proximal Gradient Step:}
   	\begin{equation*}
   	x_{k+1} = \prox_{\lambda_kh}(x_k - \lambda_k\nabla f(x_k))
   	\end{equation*}
   	\alert{Quasi-Newton Proximal Step:}
   	\begin{equation*}
   	x_{k+1} = \prox_h^{B_k}(x_k - B_k^{-1}\nabla f(x_k)),
   	\end{equation*}
   	with $B_k = \underbrace{D_k}_{diag} + \underbrace{u_k}_{\in\mathbb{R}^n}u_k^T$.
   \end{frame}
   
   \begin{frame}{Proximal Method}
   	\begin{columns}[T]
   		\begin{column}{.5\textwidth}
   			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
   			$A \in \mathbb{R}^{1500 \times 3000},\:b \in \mathbb{R}^{1500}$\\
   			$A_{ij},\:b_i\:$ \textasciitilde $\:\mathcal{N}(0,1)$, $\:\lambda = 0.1$\\
   			\vspace{28pt}
   			\resizebox{\linewidth}{!}{\input{ProxNormal.tikz}}
   		\end{column}\hfill
   		\begin{column}{.5\textwidth}
   			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
   			$A \in \mathbb{R}^{2197 \times 2197},\:b \in \mathbb{R}^{2197}$\\
   			$A$ from 7-point finite difference stencil for 3D Laplacian on a Box $\:\lambda = 1$\\
   			\vspace{10pt}
   			\resizebox{\linewidth}{!}{\input{ProxPDE.tikz}}
   		\end{column}
   	\end{columns}
   \end{frame}
   
   \begin{frame}{Proximal Method}
   	\alert{Effect of regularization parameter $\lambda$ on solution:}\\
   	\centering\includegraphics[width = 0.7\textwidth]{lambda1.png}\\
   	\centering\includegraphics[width = 0.7\textwidth]{lambda2.png}
   \end{frame}
   
   \begin{frame}{Proximal Method: Stochastic Extension}
   High-dimensional data:
   Extension to stochastic framework\\
   \vspace{10pt}
   \alert{Effect of batch size}
   	\begin{columns}[T]
   		\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 1
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_1.tikz}}
   		\end{column}\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 50
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_50.tikz}}
   		\end{column}\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 150
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_150.tikz}}
   		\end{column}
   	\end{columns}
   \end{frame}

\section{Logistic Regression: An Example}
  \begin{frame}\frametitle{Task}
    Explain what we want to do, and explain the dataset,
    and why using both SQN and Prox makes sense   
  \end{frame}
  
  \begin{frame}{eeg data}
Recording:
\begin{itemize}
\item eeg signal
\item 20 nights from healthy patient
\item each almost 10 hours
\item 1 eeg channel, 200 Hz
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{eeg.png}
\caption{eeg channels}
\label{default}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}{detection of slow oscillations}
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{so_sub1.png}
\caption{Slow oscillations for one subject}
\label{default}
\end{figure}
\end{frame}

\begin{frame}{The classification problem - roc auc metrics}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{auc.png}
\caption{AUC metrics}
\label{default}
\end{figure}
\end{frame}

  \begin{frame}\frametitle{Results}
    Nice table with SQN, SGD (no reg, L2), (Lasso,) Prox (L1) showing
    Obj. value in found optimum, CPU time, Iterations, F1 score of prediction model

    \begin{table}[t]
    \centering
      \begin{tabular}{r|c|c|c}
        \phantom 0 & \textbf{$F(\omega^*)$} & \textbf{Model Score} & \textbf{Cost}\\
      \hline \alert{No regularization}   &      & &  \\
        SGD   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
        SQN   & $0.5$  & $96\%$  & $x$ sec, $y$ AP\\
        Prox   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \hline \hline \alert{L1}        &    &   &  \\
      LASSO & $.71$  &  $55\%$ & blablabla \\
        Prox   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \hline \hline \alert{L2}        &    &   &  \\
       SGD & $.71$  &  $55\%$ & blablabla \\
        SQN   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \end{tabular}
    \end{table}
  \end{frame}

\plain{
  \resizebox{\textwidth}{!}{    
      \begin{tabular}{r|c|c|c}
        \phantom 0 & \textbf{$F(\omega^*)$} & \textbf{Model Score} & \textbf{Cost}\\
      \hline \alert{No regularization}   &      & &  \\
        SGD   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
        SQN   & $0.5$  & $96\%$  & $x$ sec, $y$ AP\\
        Prox   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \hline \hline \alert{L1}        &    &   &  \\
      LASSO & $.71$  &  $55\%$ & blablabla \\
        Prox   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \hline \hline \alert{L2}        &    &   &  \\
       SGD & $.71$  &  $55\%$ & blablabla \\
        SQN   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \end{tabular}
      }
    
    }

\section{Conclusion}

  \begin{frame}{Summary}
    \begin{center}\ccbysa\end{center}
  \end{frame}
  

  \plain{Questions?}

  \begin{frame}[allowframebreaks]\frametitle{Main References}

    \bibliography{refs}
    \bibliographystyle{abbrv}

  \end{frame}

\end{document}
