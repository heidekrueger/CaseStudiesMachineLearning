%!TEX program = xelatex
\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle, noframetitleoffset, block=fill]{m}
%\definecolor{TUMblue}{RGB}{55,55,255}
%\setbeamercolor{alerted text}{fg=TUMblue}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepackage{tikz}
\usepgfplotslibrary{dateplot}
\usepackage{caption}

\newlength\figureheight
\newlength\figurewidth
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\argmin}{argmin}

\title{Stochastic Optimization in Machine Learning}
\subtitle{Case Studies in Nonlinear Optimization}
\date{\today}
\author{F. Bauer \and S. Chambon \and R. Halbig \and S. Heidekrüger \and J. Heuke}
\institute{Technische Universität München}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.eps}}

\begin{document}

\maketitle

\plain{
  \begin{quote}
    We're not running out of data anytime soon. It's maybe the only resource that grows exponentially.
    \\
    \flushright{\alert{Andreas Weigend}}
  \end{quote}
  }


\begin{frame}
  \frametitle{Outline}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}

  \begin{frame}[t]\frametitle{Introduction: What is Machine Learning (ML) ?}
	  	Implementation of autonomously learning software for:
        \begin{itemize}
        	\item Discovery of patterns and relationships in data
        	\item Prediction of future events
        \end{itemize}
        \vspace{5pt}
        \alert{Examples:}
        \begin{columns}\hspace{5pt}
        	\begin{column}{.5\linewidth}
        		Electroencephalography (EEG)\\
        		\vspace{5pt}
        		\includegraphics[width = 0.8\linewidth]{EEG_oscillation.png}\\
        		\vspace{5pt}
        		\alert{Section 4}
        	\end{column}
        	\begin{column}{.5\linewidth}
        		Image Denoising\\
        		\vspace{5pt}
        		\includegraphics[width = 0.6\linewidth]{lena_pic.jpg}\\
        		\vspace{5pt}
        		\alert{Section 5}
        	\end{column}
        \end{columns}
  \end{frame}

  \begin{frame}\frametitle{Introduction: ML and Optimization I}
    \alert{Training} a Machine Learning model means finding optimal parameters $\omega$:

    $$ \omega^* = \argmin_{\omega} F(\omega, X, z)$$
    
    \begin{itemize}
      \item \alert{$F$}: Loss function
      \item \alert{$X$}: The training data
      \item \alert{$z$}: Training labels
    \end{itemize}   
  \end{frame}

  \begin{frame}\frametitle{Introduction: ML and Optimization II}
    After we have found $\omega^*$, we can do \alert{Prediction} on new data points:

    $$ \hat {z_i} := h(\omega^*, x_i)$$
    
    \begin{itemize}
      \item \alert{$x_i$}: new data point with \emph{unknown} label \alert{$z_i$}
      \item \alert{$h$}: hypothesis function of the ML model
    \end{itemize}   
  \end{frame}

  \begin{frame}
    \frametitle{Introduction: Challenges in Machine Learning}
      \begin{itemize}
        \item Massive amounts of training data 
        \item Construction of very large models
        \item Handling high memory/computational demands
      \end{itemize}
      \vspace{35pt}
    \centering \huge{\alert{Stochastic Methods}}
  \end{frame}
  
  \begin{frame}{Introduction: Stochastic Framework}
    $$ F(\omega) := \mathbb{E}\left[f(\omega, \xi)\right] \uncover<3->{= \frac{1}{N}\sum_{i=1}^N f(\omega, x_i, z_i)}$$
    \begin{itemize}
      \item<2-> \alert{$\xi$}: Random variable; takes the form of an input-output-pair $(x_i, z_i)$
      \item<3-> \alert{$f$}: Partial loss function corresponding to a single data point.
      \item<4-> Example loss function: $f(\omega, x_i, z_i) = |z_i - \omega^Tx_i|$ (Linear Regression)
    \end{itemize}
  \end{frame}

  \begin{frame}{Introduction: Stochastic Methods}
    \begin{columns}[T]
      \begin{column}{.5\textwidth}
        \centering \alert{Gradient Method}
        $$\min F(\omega) $$
        \phantom{0}
        
        $$\omega^{(k+1)}:= \omega^{k}-\alpha_k \nabla F(\omega^{k})$$\\
        \phantom{zeile}
        
      \end{column}\hfill
      \begin{column}{.5\textwidth}
        \centering \alert{Stochastic Gradient Descent (SGD)}
        $$\min \mathbb E \left [f(\omega, \xi)\right]$$
        \uncover<2>{
          $$\omega^{k+1}:= \omega^{k}-\alpha_k \alert{\nabla \hat F(\omega^{k})} $$
          with
          $$\alert{\nabla \hat F(\omega^{k})} := \frac{1}{b}\sum_{i\in \mathcal S_k}\nabla f(\omega^k, x_i, z_i)$$
          where $\mathcal S_k \subset [N], \quad b:=|\mathcal S_k| \ll N$\\\alert{"Mini Batch"}
        }
      \end{column}
    \end{columns}
  \end{frame}

\section{Stochastic Quasi-Newton Method (SQN)}

  \begin{frame}{Stochastic Newton Method}
  	\begin{columns}[T]
  		\begin{column}{.5\textwidth}
  			\centering \alert{Stochastic Gradient Descent}
  			$$\min \mathbb E \left [f(\omega, \xi)\right]$$
  				$$\omega^{k+1}:= \omega^{k}-\alpha_k \nabla \hat F(\omega^{k}) $$
  				
  				$$\nabla \hat F(\omega^{k}) := \frac{1}{b}\sum_{i\in \mathcal S_k}\nabla f(\omega^k, x_i, z_i)$$

  		\end{column}\hfill
  		\begin{column}{.5\textwidth}
  			\centering \alert{Stochastic Newton Method}
  			$$\min \mathbb E \left [f(\omega, \xi)\right]$$
  			\uncover<2>{
  				$$\omega^{k+1}:= \omega^{k}-\alpha_k \alert{\nabla^2 \hat F(\omega^{k})^{-1}} {\nabla \hat F(\omega^{k})} $$
  				with
  				$$\alert{\nabla^2 \hat F(\omega^{k})} := \frac{1}{b_H}\sum_{i\in \mathcal S_{H,\alert{t}}}\nabla^2 f(\omega^{\alert{t}}, x_i, z_i)$$
          where $\mathcal S_{H,\alert{t}} \subset [N], \quad b_H:=|\mathcal S_{H,\alert{t}}| \ll N$,\\
          $\alert{(t)}$ subsequence of $(k)$
          }
  		\end{column}
  	\end{columns}
  \end{frame}


  \begin{frame}
    \frametitle{SQN: Performance I}

      \begin{columns}[T]
      \begin{column}{.5\textwidth}
        \resizebox{\linewidth}{!}{\input{'EEG Sample Objective vs. Iterations.tikz'}}
      \end{column}\hfill
      \begin{column}{.5\textwidth}
        \resizebox{\linewidth}{!}{\input{'EEG Fixed Subset Objective vs. Iterations.tikz'}}
      \end{column}
    \end{columns}
    \center{Performance on Logistic Regression, Problem size: $69550 \times 600$\\
    \tiny{Armijo-stepsizes, Further SQN-parameters: $L=10$, $M=5$}}
  \end{frame}

  \begin{frame}
    \frametitle{SQN: Performance II}
    \center{\resizebox{0.8\linewidth}{!}{\input{'EEG Fixed Subset Objective vs. Accessed Data Points.tikz'}}}
    \center{Performance on Logistic Regression, Problem size: $69550 \times 600$\\
    \tiny{Armijo-stepsizes, Further SQN-parameters: $L=10$, $M=5$}}
  \end{frame}

  \begin{frame}\frametitle{SQN: Main Results}
    \begin{itemize}
      \item Can be faster than SGD on appropriate Datasets
      \item Requires tedious, manual tuning of hyperparameters to be efficient!
      \item Convergence conditions
    \end{itemize}
  
  \end{frame}

 \section{Proximal Method}

   \begin{frame}{Proximal Method: Basic Theory}
       \begin{flalign*}
       	\text{\alert{Problem}}&&
       	\min_x &\;F(x) := \underbrace{f(x)}_{smooth} \quad + \underbrace{h(x)}_{non-smooth}&
       \end{flalign*}
       
       \begin{flalign*}
       	\text{\alert{Proximity Operator}}&&\prox_h(v) = &\underset{x}{\argmin} \; \bigl( h(x) + \frac{1}{2} \lVert x - v \rVert^2_2 \bigr)&
       \end{flalign*}
		\begin{figure}[t] 
			
			\centering\includegraphics[width = 0.5\textwidth]{prox_boyd.jpg}
			\caption{\footnotesize Evaluating a proximal operator at various points. \textit{N Parikh, S Boyd, Proximal Methods,
					Foundations and Trends in Optimization 1, 2014}}
		\end{figure} 	
   \end{frame}
   
   \begin{frame}{Proximal Method: Zero-Memory Rank-One Update}
   	\alert{Traditional Proximal Gradient Step:}
   	\begin{equation*}
   	x_{k+1} = \prox_{\lambda_kh}(x_k - \lambda_k\nabla f(x_k))
   	\end{equation*}
   	\alert{Quasi-Newton Proximal Step:}
   	\begin{equation*}
   	x_{k+1} = \prox_h^{B_k}(x_k - B_k^{-1}\nabla f(x_k)),
   	\end{equation*}
   	with $B_k = \underbrace{D_k}_{diag} + \underbrace{u_k}_{\in\mathbb{R}^n}u_k^T$.\\
   	\vspace{5pt}
   	A zero-memory approach is used
   \end{frame}
   
   \begin{frame}{Proximal Method: Performance I}
   	\begin{columns}[T]
   		\begin{column}{.5\textwidth}
   			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
   			$A \in \mathbb{R}^{1500 \times 3000},\:b \in \mathbb{R}^{1500}$\\
   			$A_{ij},\:b_i\:$ \textasciitilde $\:\mathcal{N}(0,1)$, $\:\lambda = 0.1$\\
   			\vspace{15pt}
   			\resizebox{\linewidth}{!}{\input{ProxNormal.tikz}}
   			\begin{center}
   				\hspace{-3pt}
   				\scalebox{0.85}{
   					\begin{tabular}{|c|c|c|c|}
   						\hline                       
   						&\tiny \textbf{0SR1} & \tiny \textbf{ProxGrad} &  \tiny \textbf{L-BFGS-B} \\ \hline
   						\tiny \textbf{Iterations} &\tiny  1,822	& \tiny 135,328 & \tiny 1,989 \\	\hline  
   						\tiny \textbf{Run-Time}&\tiny 68 s & \tiny 1,144 s &\tiny 56 s  \\ \hline
   						
   					\end{tabular}
   				}
   			\end{center}
   		
   		\end{column}\hfill
   		\begin{column}{.5\textwidth}
   			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
   			$A \in \mathbb{R}^{2197 \times 2197},\:b \in \mathbb{R}^{2197}$\\
   			$A$: \small Discretization of 3D Laplacian\\
   			\normalsize$\lambda = 1$\\
   			\vspace{8pt}
   			\resizebox{\linewidth}{!}{\input{ProxPDE.tikz}}
   			\begin{center}
   				\hspace{5pt}
   				\scalebox{0.85}{
   					\begin{tabular}{|c|c|c|c|}
   						\hline                       
   						&\tiny \textbf{0SR1} & \tiny \textbf{ProxGrad} &  \tiny \textbf{L-BFGS-B} \\ \hline
   						\tiny \textbf{Iterations}& \tiny  7	& \tiny 18 & \tiny 10 \\	\hline  
   						\tiny \textbf{Run-Time}& \tiny 0.037 s &\tiny 0.004 s &\tiny 0.022 s\\ \hline
   						
   					\end{tabular}
   				}
   			\end{center}
   		\end{column}
   	\end{columns}
   \end{frame}
   
   \begin{frame}{Proximal Method: Stochastic Extension}
   High-dimensional data:
   Extension to stochastic framework\\
   \vspace{25pt}
   \centering\alert{Effect of batch size}
   	\begin{columns}[T]
   		\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 1
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_1.tikz}}
   		\end{column}\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 50
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_50.tikz}}
   		\end{column}\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 150
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_150.tikz}}
   		\end{column}
   	\end{columns}
   \end{frame}
   
   \begin{frame}{Proximal Method: Main Results}
   	\begin{itemize}
   		\item Superior results to standard proximal gradient
   		\item Competitive with other standard methods
   		\item Extension to stochastic framework possible
   		\item Applicable to large-scale problems
   	\end{itemize}
   \end{frame}

\section{Classification}

\plain{Electroencephalography (EEG)\\
	\vspace{10pt}
	\alert{How deep is your sleep?}
	\vspace{15pt}\\
	\includegraphics[width=0.7\textwidth]{EEG_oscillation.png}\\
	\vspace{15pt}
	\small Sleeping patient / 20 nights of EEG recordings\\
	\small Predict next slow wave
	}

  
    	\begin{frame}{Classification: Results for SQN}
    		\begin{center}
    		\begin{tabular}{c|c|c}
    			
    			
    			\textbf{Batch-size} &  1000, 1000 &  500, 500 \\  \hline
    			\textbf{ Mean Score} &   0.8	&  0.8  \\	\hline  
    			\textbf{ Std} &  0.007 &  0.006 \\ \hline
    			\textbf{ Running Time} &  65 s &  31 s  \\ \hline 
    			\textbf{ M} &  5 &  5  \\ \hline
    			\textbf{ L} &  10 &  10  \\ 
    			
    		\end{tabular}
    \end{center}
    \end{frame}
    \begin{frame}\frametitle{Classification: Results for 0SR1}
    	\begin{center}
    		\begin{tabular}{c|c|c||c|c}
    			
    			&\textbf{ $\lambda$=0.1 } &\textbf{  $\lambda$=0.01}&\textbf{ $\lambda$=0.1 } &\textbf{ $\lambda$=0.01} \\ \hline
    			\textbf{Batch-size} & 100 &  100&  1000 &  1000\\  \hline
    			\textbf{ Mean Score} &   0.8	&  0.67 & 0.8 & 0.8\\	\hline  
    			\textbf{ Std} &  0.01 &  0.14 &  0.01& 0.016\\ \hline
    			\textbf{ Running Time} &  63 s &  45 s & 68 s& 69 s \\ 
    			
    		\end{tabular}
    	\end{center}
    \end{frame}

\section{Dictionary Learning}
\plain{Image Denoising\\
	\vspace{10pt}
	\alert{Can we recover the image?}
	\vspace{15pt}\\
	\includegraphics[width=0.4\textwidth]{lena_pic.jpg}\\
	\vspace{15pt}
	\small Image is partially destroyed\\
	\small Reconstruct image
}
\begin{frame}{Dictionary Learning: Basic Theory}
	\center Well-known machine learning model:
	\begin{equation*}
	\min_{D, \alpha} \frac{1}{N} \sum_{i=1}^N \|\underbrace{ x_i - D \alpha_i }_{\text{\alert{a) SQN}}}\|_2^2 + \underbrace{\lambda \| 
		\alpha_i \|_1}_{\text{\alert{b) Prox}}}
	\end{equation*}
	\center \alert{ 2-phase optimization problem}
	\begin{itemize}
		\item[1.] Update "dictionary" 
		\item[2.] Induce sparsity
	\end{itemize}
	$\Rightarrow$ Example: Reconstruction of partially distorted images
\end{frame}

\begin{frame}{Dictionary Learning in Image Reconstruction I}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.8\textwidth]{lena_noisy.png}
		\caption{Noisy image}
	\end{figure}
\end{frame}

\begin{frame}{Dictionary Learning in Image Reconstruction II}
	\begin{figure}[h!]
		\centering
		\includegraphics[width = 0.8\linewidth]{lena_reconstructed.png}
		\caption{Reconstructed image}
	\end{figure}
\end{frame}




\section{Conclusion}

  \begin{frame}{Summary}
    \begin{itemize}
    	\item Large amounts of data
    	\item Need for stochastic algorithms
    	\item Second order methods to improve speed
    	\item For smooth and non-smooth problems
    	\item Good performance of implementation on various problems
    \end{itemize}
    \phantom{\cite{SQN}\cite{becker2012quasi}\cite{mairal2010online}\cite{parikh2013proximal}}
  \end{frame}
\plain{
	
	
	\begin{center}
		
		\includegraphics[width = 0.7\linewidth]{noisy_picture.png}\\
		
		
		
		\includegraphics[width = 0.7\linewidth]{reconstructed.png}\\
	\end{center}
}  

  \plain{Questions?}

  \begin{frame}[allowframebreaks]


    {\footnotesize{
    \bibliographystyle{abbrv}
    \bibliography{refs}
    }
    }
    %\bibliography{refs}
    %\bibliographystyle{abbrv}

  \end{frame}

\end{document}
