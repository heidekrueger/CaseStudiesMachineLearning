%!TEX program = xelatex
\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle, noframetitleoffset, block=fill]{m}
%\definecolor{TUMblue}{RGB}{55,55,255}
%\setbeamercolor{alerted text}{fg=TUMblue}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepackage{tikz}
\usepgfplotslibrary{dateplot}
\usepackage{caption}

\newlength\figureheight
\newlength\figurewidth
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\argmin}{argmin}

\title{Stochastic Optimization in Machine Learning}
\subtitle{Case Studies in Nonlinear Optimization}
\date{\today}
\author{F. Bauer \and S. Chambon \and R. Halbig \and S. Heidekrüger \and J. Heuke}
\institute{Technische Universität München}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.eps}}

\begin{document}

\maketitle

\plain{
  \begin{quote}
    We're not running out of data anytime soon. It's maybe the only resource that grows exponentially.
    \\
    \flushright{\alert{Andreas Weigend}}
  \end{quote}
  }


\begin{frame}
  \frametitle{Outline}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}

  \begin{frame}[t]\frametitle{Introduction: What is Machine Learning (ML) ?}
	  	Implementation of autonomously learning software for:
        \begin{itemize}
        	\item Discovery of patterns and relationships in data
        	\item Prediction of future events
        \end{itemize}
        \vspace{5pt}
        \alert{Examples:}
        \begin{columns}
        	\begin{column}{.5\linewidth}
        		Electroence-phalography (EEG)\\
        		\includegraphics[width = 0.8\linewidth]{eeg_pic.jpg}\\
        		\alert{Section 4}
        	\end{column}\hspace{-10pt}
        	\begin{column}{.5\linewidth}
        		Image Denoising\\
        		\includegraphics[width = 0.8\linewidth]{lena_pic.jpg}\\
        		\alert{Section 5}
        	\end{column}
        \end{columns}
  \end{frame}

  \begin{frame}\frametitle{Introduction: ML and Optimization I}
    \alert{Training} a Machine Learning model means finding optimal parameters $\omega$:

    $$ \omega^* = \argmin_{\omega} F(\omega, X, z)$$
    
    \begin{itemize}
      \item \alert{$F$}: Loss function of chosen ML-model
      \item \alert{$X$}: The training data ($N:=\#$samples $\times$ $\#$features matrix)
      \item \alert{$z$}: Training labels (only in classification models; vector of size $N$)
      
      \item The dimension $n$ of $\omega$ is model dependent, often $\#$features$+1$
    \end{itemize}   
  \end{frame}

  \begin{frame}\frametitle{Introduction: ML and Optimization II}
    After we have found $\omega^*$, we can do \alert{Prediction} on new data points:

    $$ \hat {z_i} := h(\omega^*, x_i)$$
    
    \begin{itemize}
      \item \alert{$x_i$}: new data point with \emph{unknown} label \alert{$z_i$}
      \item \alert{$h$}: hypothesis function of the ML model
    \end{itemize}   
  \end{frame}

  \begin{frame}
    \frametitle{Introduction: Challenges in Machine Learning}
      \begin{itemize}
        \item Massive amounts of training data 
        \item Construction of very large models
        \item Handling high memory/computational demands
      \end{itemize}
      \vspace{36pt}
      
    \centering \large{Ansatz: \alert{Stochastic Methods}}
  \end{frame}
  
  \begin{frame}{Introduction: Stochastic Framework}
    $$ F(\omega) := \mathbb{E}\left[f(\omega, \xi)\right] \uncover<3>{= \frac{1}{N}\sum_{i=1}^N f(\omega, x_i, z_i)}$$
    \begin{itemize}
      \item<2-> \alert{$\xi$}: Random variable; takes the form of an input-output-pair $(x_i, z_i)$
      \item<3-> \alert{$f$}: Partial loss function corresponding to a single data point.
    \end{itemize}
  \end{frame}

  \begin{frame}{Introduction: Stochastic Methods}
    \begin{columns}[T]
      \begin{column}{.5\textwidth}
        \centering \alert{Gradient Method}
        $$\min F(\omega) $$

        \uncover<2->{
        $$\omega^{(k+1)}:= \omega^{k}-\alpha_k \nabla F(\omega^{k})$$\\
        \phantom{zeile}
        }
      \end{column}\hfill
      \begin{column}{.5\textwidth}
        \centering \alert{Stochastic Gradient Descent}
        $$\min \mathbb E \left [f(\omega, \xi)\right]$$
        \uncover<3>{
          $$\omega^{k+1}:= \omega^{k}-\alpha_k \alert{\nabla \hat F(\omega^{k})} $$
          with
          $$\alert{\nabla \hat F(\omega^{k})} := \frac{1}{b}\sum_{i\in \mathcal S_k}f(\omega, x_i, z_i)$$
          where $\mathcal S_k \subset [N], \quad b:=|\mathcal S_k| \ll N$\\\alert{"Mini Batch"}
        }
      \end{column}
    \end{columns}
  \end{frame}

\section{SQN: A Stochastic Quasi-Newton Method}

  \begin{frame}\frametitle{Stochastic Quasi-Newton Method (SQN)}
      \begin{itemize}
        \item \alert{Stochastically} use second-order information
        \item Based on BFGS-method.
        \
        \item Basic idea: $$ \omega^{k+1} = \omega^{k} - \alpha_k \alert{H_t} \nabla \hat F(\omega^{k})$$
        
        \item $t$ running on slower time-scale than $k$. 
        \item $H_t$ update in $\mathcal O(n)$ time and constant memory, using several tricks
      \end{itemize}
  \end{frame}

  \subsection*{Behavior}

  \begin{frame}
    \frametitle{Behavior I}

      \begin{columns}[T]
      \begin{column}{.5\textwidth}
        \resizebox{\linewidth}{!}{\input{'EEG Sample Objective vs. Iterations.tikz'}}
      \end{column}\hfill
      \begin{column}{.5\textwidth}
        \resizebox{\linewidth}{!}{\input{'EEG Fixed Subset Objective vs. Iterations.tikz'}}
      \end{column}
    \end{columns}
    \center{Performance on EEG Dataset, Problem size: $69550 \times 600$\\
    \tiny{Armijo-stepsizes, Further SQN-parameters: $L=10$, $M=5$}}
  \end{frame}

  \begin{frame}
    \frametitle{Behavior II}
    \resizebox{\linewidth}{!}{\input{'EEG Fixed Subset Objective vs. Accessed Data Points.tikz'}}
    \center{Performance on EEG Dataset, Problem size: $69550 \times 600$\\
    \tiny{Armijo-stepsizes, Further SQN-parameters: $L=10$, $M=5$}}
  \end{frame}

  \begin{frame}\frametitle{Results}
    \begin{itemize}
      \item Can be faster than SGD on appropriate Datasets
      \item Requires tedious, manual tuning of hyperparameters to be efficient!
    \end{itemize}
  
  \end{frame}

 \section{Proximal Method}

   \begin{frame}{Proximal Method}
       \begin{flalign*}
       	\text{\alert{Problem}}&&
       	\min_x &\;F(x) := \underbrace{f(x)}_{smooth} \quad + \underbrace{h(x)}_{non-smooth}&
       \end{flalign*}
       
       \begin{flalign*}
       	\text{\alert{Proximity Operator}}&&\prox_f(v) = &\underset{x}{\argmin} \; \bigl( f(x) + \frac{1}{2} \lVert x - v \rVert^2_2 \bigr)&
       \end{flalign*}
		\begin{figure}[t] 
			
			\centering\includegraphics[width = 0.5\textwidth]{prox_boyd.jpg}
			\caption{\footnotesize Evaluating a proximal operator at various points. \textit{N Parikh, S Boyd, Proximal Methods,
					Foundations and Trends in Optimization 1, 2014}}
		\end{figure} 	
   \end{frame}
   
   \begin{frame}{Proximal Method}
   	\alert{Traditional Proximal Gradient Step:}
   	\begin{equation*}
   	x_{k+1} = \prox_{\lambda_kh}(x_k - \lambda_k\nabla f(x_k))
   	\end{equation*}
   	\alert{Quasi-Newton Proximal Step:}
   	\begin{equation*}
   	x_{k+1} = \prox_h^{B_k}(x_k - B_k^{-1}\nabla f(x_k)),
   	\end{equation*}
   	with $B_k = \underbrace{D_k}_{diag} + \underbrace{u_k}_{\in\mathbb{R}^n}u_k^T$.
   \end{frame}
   
   \begin{frame}{Proximal Method}
   	\begin{columns}[T]
   		\begin{column}{.5\textwidth}
   			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
   			$A \in \mathbb{R}^{1500 \times 3000},\:b \in \mathbb{R}^{1500}$\\
   			$A_{ij},\:b_i\:$ \textasciitilde $\:\mathcal{N}(0,1)$, $\:\lambda = 0.1$\\
   			\vspace{15pt}
   			\resizebox{\linewidth}{!}{\input{ProxNormal.tikz}}
   			\begin{center}
   				\hspace{-3pt}
   				\scalebox{0.85}{
   					\begin{tabular}{|c|c|c|c|}
   						\hline                       
   						&\tiny \textbf{0SR1} & \tiny \textbf{ProxGrad} &  \tiny \textbf{L-BFGS-B} \\ \hline
   						\tiny \textbf{Iterations} &\tiny  1,822	& \tiny 135,328 & \tiny 1,989 \\	\hline  
   						\tiny \textbf{Run-Time}&\tiny 68 s & \tiny 1,144 s &\tiny 56 s  \\ \hline
   						
   					\end{tabular}
   				}
   			\end{center}
   		
   		\end{column}\hfill
   		\begin{column}{.5\textwidth}
   			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
   			$A \in \mathbb{R}^{2197 \times 2197},\:b \in \mathbb{R}^{2197}$\\
   			$A$: \small Discretization of 3D Laplacian\\
   			\normalsize$\lambda = 1$\\
   			\vspace{8pt}
   			\resizebox{\linewidth}{!}{\input{ProxPDE.tikz}}
   			\begin{center}
   				\hspace{5pt}
   				\scalebox{0.85}{
   					\begin{tabular}{|c|c|c|c|}
   						\hline                       
   						&\tiny \textbf{0SR1} & \tiny \textbf{ProxGrad} &  \tiny \textbf{L-BFGS-B} \\ \hline
   						\tiny \textbf{Iterations}& \tiny  7	& \tiny 18 & \tiny 10 \\	\hline  
   						\tiny \textbf{Run-Time}& \tiny 0.037 s &\tiny 0.004 s &\tiny 0.022 s\\ \hline
   						
   					\end{tabular}
   				}
   			\end{center}
   		\end{column}
   	\end{columns}
   \end{frame}
   
   \begin{frame}{Proximal Method: Stochastic Extension}
   High-dimensional data:
   Extension to stochastic framework\\
   \vspace{10pt}
   \alert{Effect of batch size}
   	\begin{columns}[T]
   		\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 1
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_1.tikz}}
   		\end{column}\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 50
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_50.tikz}}
   		\end{column}\hspace{-16pt}
   		\begin{column}{.3\textwidth}
   			\hspace{30pt} \scriptsize Batch size = 150
   			\vspace{10pt}
   			\resizebox{1.18\linewidth}{!}{\input{StochProx_150.tikz}}
   		\end{column}
   	\end{columns}
   \end{frame}

\section{Logistic Regression: An Example}

\plain{Electroencephalography (EEG)\\
	\vspace{10pt}
	\alert{How deep is your sleep?}
	\vspace{15pt}\\
	\includegraphics[width=0.7\textwidth]{EEG_oscillation.png}\\
	\vspace{15pt}
	\small Sleeping patient / 20 nights of EEG recordings\\
	\small Predict next slow wave
	}

\begin{frame}{EEG: Logistic Regression}
	
\end{frame}

  \begin{frame}\frametitle{Results}
    Nice table with SQN, SGD (no reg, L2), (Lasso,) Prox (L1) showing
    Obj. value in found optimum, CPU time, Iterations, F1 score of prediction model

    \begin{table}[t]
    \centering
      \begin{tabular}{r|c|c|c}
        \phantom 0 & \textbf{$F(\omega^*)$} & \textbf{Model Score} & \textbf{Cost}\\
      \hline \alert{No regularization}   &      & &  \\
        SGD   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
        SQN   & $0.5$  & $96\%$  & $x$ sec, $y$ AP\\
        Prox   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \hline \hline \alert{L1}        &    &   &  \\
      LASSO & $.71$  &  $55\%$ & blablabla \\
        Prox   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \hline \hline \alert{L2}        &    &   &  \\
       SGD & $.71$  &  $55\%$ & blablabla \\
        SQN   & $0.01$  & $96\%$  & $x$ sec, $y$ AP\\
      \end{tabular}
    \end{table}
  \end{frame}

\section{Dictionary Learning}
\plain{Dictionary Learning\\
	\vspace{10pt}
	\alert{Can we recover the image?}
	\vspace{15pt}\\
	\includegraphics[width=0.4\textwidth]{lena_pic.jpg}\\
	\vspace{15pt}
	\small Image is partially destroyed\\
	\small Reconstruct image
}
\begin{frame}{Dictionary Learning}
	bla
\end{frame}

\section{Conclusion}

  \begin{frame}{Summary}
    \begin{center}\ccbysa\end{center}
  \end{frame}
  

  \plain{Questions?}

  \begin{frame}[allowframebreaks]\frametitle{Main References}

    \bibliography{refs}
    \bibliographystyle{abbrv}

  \end{frame}

\section{Appendix}
\begin{frame}{Proximal Method}
	\centering\includegraphics[width = 0.9\textwidth]{ProxNormal_full.png}
\end{frame}
\begin{frame}{Proximal Method}
	\begin{columns}[T]
		\begin{column}{.5\textwidth}
			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
			$A \in \mathbb{R}^{1500 \times 3000},\:b \in \mathbb{R}^{1500}$\\
			$A_{ij},\:b_i\:$ \textasciitilde $\:\mathcal{N}(0,1)$, $\:\lambda = 0.1$\\
			\vspace{28pt}
			\resizebox{\linewidth}{!}{\input{convNormal.tikz}}
		\end{column}\hfill
		\begin{column}{.5\textwidth}
			$F(x) = \lVert Ax - b \rVert + \lambda \lVert x \rVert_1$\\
			$A \in \mathbb{R}^{2197 \times 2197},\:b \in \mathbb{R}^{2197}$\\
			$A$: \small Discretization of 3D Laplacian\\
			\normalsize$\lambda = 1$\\
			\vspace{10pt}
			\resizebox{\linewidth}{!}{\input{convPDE.tikz}}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}\frametitle{SQN: CPU Time}
    \resizebox{\linewidth}{!}{\input{'EEG Fixed Subset Objective vs. CPU time.tikz'}}

\begin{frame}{Proximal Method}
	\centering\includegraphics[width=0.9\textwidth]{lambda1.png}\\
	\centering\includegraphics[width=0.9\textwidth]{lambda2.png}

\end{frame}
\end{document}
