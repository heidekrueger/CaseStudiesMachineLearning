
class SQN_Solver:
	"""
	SQN Solver class
	contains optimization parameters. Call using solve(w_start=None) method.
	"""
	def __init__(self):
		self.params = {"w1": None, 
					 "dim": None, 
					 "M": 10,
					 "L": 1.0, 
					 "beta":1, 
					 "batch_size": 1, 
					 "batch_size_H": 1,
					 "max_iter": 1e4, 
					 "debug": False, 
					 "sampleFunction": sampleFunction}
		#chooseSample = stochastic_tools.sample_batch
		
		self.w = None
		self.f = None
		self.g = None
		
	def set_params(self, params):
		for key in params:
			try:
				self.params[key] = params[key]
			except KeyError, m:
				print m
	def get_params(self):
		return self.params
	
	def solve_spaeter(self, w1):
		if type(w1) == type(np.zeros(1).flat):
			self.w = np.array([i for i in w1])
		else:
			self.w = w1
			
		if type(w1) == type(np.zeros(1).flat):
			for i in range(len(self.w)):
				w1[i] = self.w[i]
		
	def solve(self, f, g, w1 = None, dim = None):
		
		assert self.M > 0, "Memory Parameter M must be a positive integer!"
		
		assert w1 != None and (dim != None or self.dim != None), "Please privide either a starting point or the dimension of the optimization problem!"
		
		self.f = f
		self.g = g
		self.w = w
		
		# dimensions
		nSamples = len(X)
		nFeatures = len(X[0])
		
		if w1 == None:  
		    w1 = np.zeros(dim)
		#    w1[0] = 3
		#    w1[0] = 4
		self.w = w1

		
		#Set wbar = wPrevious = 0
		self.wbar = w1
		self.wPrevious = w1
		if debug: print self.w.shape
		# step sizes alpha_k
		self.alpha_k = self.beta
		#alpha = lambda k: beta/(k + 1)

		self.s, self.y = deque(), deque()
		
		# accessed data points
		t = -1
		H = None
		for k in itertools.count():
			self.w = solve_step(self, k, X, z)
			
		if iterations < self.max_iter:
			print "Terminated successfully!" 
		print "Iterations:\t\t", iterations
		return self.w

	def solve_step(self, k, X, z):
		
		if self.debug: print "Iteration", k
		
		# Draw mini batch
		X_S, z_S= self.chooseSample(w=self.w, X=X, z=z, b = self.batch_size)
		
		# Try a second time if sample is empty
		if len(X_S) == 0: X_S, z_S= self.chooseSample(w=self.w, X=X, z=z, b = self.batch_size)
		
		# Termination Condition
		return len(X_S) == 0 or self.hasTerminated(f , self.stochastic_gradient(self.g, self.w, X_S, z_S), self.w , k, max_iter = self.max_iter)
			
		
		# Determine search direction
		if k <= 2*L:  	search_direction = -self.stochastic_gradient(self.g, w, X_S, z_S)
		else:	   	search_direction = -H.dot(self.stochastic_gradient(self.g, w, X_S, z_S))
		if debug: 		print "Direction:", search_direction.T
	
		# Compute step size alpha
		f_S = lambda x: self.f(x, X_S, z_S) if z is not None else self.f(x, X_S)
		g_S = lambda x: self.stochastic_gradient(self.g, x, X_S, z_S)
		self.alpha_k = armijo_rule(f_S, g_S, self.w, search_direction, start = self.beta, beta=.5, gamma= 1e-2 )
		self.alpha_k = max([alpha_k, 1e-5])
		    
		if debug: print "f\n", f_S(w)
		if debug: print "w\n", self.w
		if debug: print "alpha", self.alpha_k
		
		# Perform update
		wPrevious = w
		w = w + np.multiply(alpha_k, search_direction)
		wbar += w
		
		# compute Correction pairs every L iterations
		if k%L == 0:
			t += 1
			wbar /= float(L) 
			if t>0:
				#choose a Sample S_H \subset [nSamples] to define Hbar
				X_SH, y_SH = chooseSample(w, X, z, b = batch_size_H)
				
				(s_t, y_t) = correctionPairs(g, w, wPrevious, X_SH, y_SH)
				
				if debug: print "correction shapes", s_t, y_t
				s.append(s_t)
				y.append(y_t)
				if len(s) > M:
					s.popleft()
					y.popleft()
					
				H = getH(s, y)
				
			wbar = np.zeros(dim)
			
		return False




sqn = SQN_Solver()
sqn.set_max_iter(500)
sqn.set_params({"max_iter": 500, "M": 5})

self.attributes["max_iter"] = 500

def solveSQN(f, g, X, z = None, w1 = None, dim = None, M=10, L=1.0, beta=1, batch_size = 1, batch_size_H = 1, max_iter = 1e4, debug = False, sampleFunction = None):
	"""
	
	"""
	