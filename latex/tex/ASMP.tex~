
\newpage

\subsection{Soft Margin Rosenblatt's Perceptron}

\ba
\min_\alpha \left\{ \half c \alpha^T \Sigma \alpha \; : \; 1 - A\alpha - B \beta \leq 0\right\}
\ea
Lagrange:
\ba
\max_\lambda \left\{ \inf_\alpha \{ \half c \alpha^T \Sigma \alpha + \lambda^T1 - \lambda^T A \alpha - \lambda^T B \beta\}\right\}
\ea
Take $\lambda := \tilde\lambda e_i $ such that $e_i^T A \alpha - e_i^T B \beta \leq 1$.
Define update rule
\ba
\alpha^{(k+1)} 
&= (1 - \eta c) \alpha^{(k)} + \eta \tilde\lambda A^T e_i \\
\beta^{(k+1)}
&= \beta^{(k)} + \eta \tilde\lambda B^T e_i 
\ea
\subsection*{Lemma}
If $\norm{A^T e_i} \leq D$ for every unit vector $e_i$ and $c \eta < 1$, then from the above update rule, one gets $\norm{\alpha^{(k)}} \leq \frac{1 - [1 - c \eta]^k}{c} \, \tilde\lambda \, D \leq \frac{\tilde\lambda \, D}{c}$ for all k. 

\subsubsection*{Proof:}

Take any $A^T e_i$ and assume it is misclassified in the $i(k)$th step. We use the Cauchy-Schwartz-inequality in order to obtain for the update
$k= 2$:
\ba
\norm{\alpha^{(2)}} &\leq [1 - c \eta] \norm{\alpha^{(1)}} + \eta \tilde\lambda \norm{A^T e_i} 
\leq [1 - c \eta] \left( [1 - c \eta] \norm{\alpha^{(0)}} + \eta \tilde\lambda D \right) + \eta \tilde\lambda D \\
& = [2 - c \eta] \eta \tilde\lambda D 
= \frac{1 - [1 - c \eta]^2}{1 - [1 - c \eta]} \eta \tilde\lambda D = \frac{1 - [1 - c \eta]^2}{c} \tilde\lambda D \leq \frac{ \tilde\lambda D}{c}
\ea
since $\alpha^{(0)} = 0$. Now assume the assertion is true for all $l \leq k-1$. Then for $l = k$ we get:
\ba
\norm{\alpha^{(k)}} 
&\leq [1 - c \eta] \norm{\alpha^{(k-1)}} + \eta \tilde\lambda \norm{A^Te_i} \leq [1 - c \eta]\frac{1 - [1 - c \eta]^{k-1}}{1 - [1 - c\eta]} \, \eta \tilde\lambda \, D  \\
&\leq [1 - c \eta] \sum_{l = 0}^{k-1} [1-c\eta]^l \eta \tilde\lambda D + \eta \tilde\lambda D 
= \sum_{l = 0}^{k} [1-c\eta]^l \eta \tilde\lambda D \\
& = \frac{1 - [1 - c \eta]^k}{c} \, \tilde\lambda \, D \leq \frac{\tilde\lambda \, D}{c}
\ea
q.e.d

\subsection*{Corollary}
In the above setting we have $ -e_i^T \alpha^{(k)} \leq - \frac{\tilde\lambda \, D}{c}$ for all k.

\subsubsection*{Proof:}
From the above Lemma we know that $\norm{\alpha^{(k)}} \leq \frac{\tilde\lambda \, D}{c}$ and therefore:
\ba
-e_i \alpha^{(k)} \leq |e_i^T\alpha^{(k)}| \leq \norm{\alpha^{(k)}} \leq \frac{\tilde\lambda \, D}{c}.
\ea
q.e.d \\


\subsection{Theorem}

Let $1 < D := \max_i \norm{x_i}_2 < \infty$ and assume that the optimization problem \ref{relaxed_objective} has got a solution. 
Then the Soft Margin Rosenblatt Algorithm converges for $0 < c < 1$.

\subsubsection*{Proof:}

\begin{itemize}
\item[First Part]
\end{itemize}

\ba
\norm{\alpha^{(k+1)}}^2 
&\leq \norm{\alpha^{(k)}}^2 + 2 \eta \tilde\lambda e_i^TA\alpha^{(k)} + \eta^2 \tilde\lambda^2 e_i^TA^TAe_i - 2 c \eta^2 \tilde\lambda e_i^TA\alpha^{(k)}\\
\norm{\beta^{(k+1)}}^2 
&= \norm{\beta^{(k)}}^2 + 2 \eta \tilde\lambda e_i^T B  \beta^{(k)} + \eta^2 \tilde\lambda^2 e_i^T B^T B e_i
\ea
since $(1-c\eta) \leq 1$. Now we use that $A \alpha^{(k)} + B \beta^{(k)} \leq 1$:
\ba
\norm{\alpha^{(k+1)}}^2 + \norm{\beta^{(k+1)}}^2 
&\leq \norm{\alpha^{(k)}}^2 + \norm{\beta^{(k)}}^2 
+ 2 \eta \tilde\lambda [1 + \tilde\lambda \, D] + \eta^2 \tilde\lambda^2 e_i^T(A^TA + B^T B) e_i \\
&\leq (k + 1) \eta \tilde\lambda [ 2 + 2 \tilde\lambda D + \eta \tilde\lambda e_i^T(A^TA + B^T B) e_i]
\ea

\begin{itemize}
\item[Second Part]
\end{itemize}
\ba
\left|[u^T v^T] \begin{pmatrix} \alpha^{(k+1)} \\ \beta^{(k+1)} \end{pmatrix} \right|^2
&\leq \norm u^2 \norm{\alpha^{(k+1)}}^2 + \norm v^2 \norm{\beta^{(k+1)}}^2 \\
& \leq (k + 1) \norm v^2 \eta \tilde\lambda [ 2 + 2 \tilde\lambda D + \eta \tilde\lambda e_i^T(A^TA + B^T B) e_i]
\ea

\begin{itemize}
\item[Third Part]
\end{itemize}

\ba
u^T \alpha^{(k+1)} + v^T\beta^{(k+1)}
&= u^T \alpha^{(k)} + v^T \beta^{(k)} - c \eta u^T \alpha^{(k)} + \eta \tilde\lambda u^T A e_i + \eta \tilde\lambda v^T B e_i \\
&\geq u^T \alpha^{(k)} -  \eta \tilde\lambda D+ \eta \tilde\lambda (1 + \gamma) \\ 
& \geq (k+1) \eta \tilde\lambda [ 1 + \gamma -  D]
\ea

\begin{itemize}
\item[Fourth Part]
\end{itemize}
\ba
(k+1) \eta \tilde\lambda [ 1 + \gamma -  D] 
\leq \sqrt{k + 1} \norm v \sqrt{\eta \tilde\lambda [ 2 + 2 \tilde\lambda D + \eta \tilde\lambda e_i^T(A^TA + B^T B) e_i]}
\ea
Or:
\ba
k + 1 \leq \norm v ^2\frac{\eta \tilde\lambda [ 2 + 2 \tilde\lambda D + \eta \tilde\lambda e_i^T(A^TA + B^T B) e_i]}{
\ea
