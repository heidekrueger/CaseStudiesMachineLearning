
\section{Expectation Maximization Algorithm}

\begin{itemize}
\item[x] observable
\item[y] hidden variable
\item[$\theta$] parameterset
\end{itemize}

\ba
p(x,y| \theta) 
&= p(x|\theta) \frac{p(x,y,\theta)}{p(x|\theta) p(\theta)} \\
\Leftrightarrow
p(x,y| \theta) 
&= p(x|\theta) p(y|x,\theta) \\
\Leftrightarrow 
\log p(x| \theta) 
&=  \log p(x,y|\theta) - \log p(y|x,\theta) \\
\ea
multiplying with $p(y|x, \hat\theta)$ and marginalization, since $p(x,\theta)$ is independent of $y$:
\ba
-\int_Y p(y|x,\hat\theta) \log p(x|\theta) dy 
&= -\int_Y p(y|x,\hat\theta) \log p(x,y|\theta) dy 
-\left(-\int_Y p(y|x,\hat\theta) \log p(y|x,\theta) dy \right) \\
\Leftrightarrow \;\;
\log p(x|\theta) &=: Q(\hat\theta, \theta) + H(\hat\theta,\theta) \\
\text{observable information } &= \text{ complete information + hidden information }
\ea


BETTER:
\ba
\log p(x|\theta)
&= \int_Y p(y|x,\hat\theta) \left[ \log p(x,y|\theta)  
- \log p(y|x,\theta) \right] dy \\
&= \int_Y \frac{p(y|\hat\theta) p(x | y, \hat\theta)} { p(x| \hat\theta)}
 \left[ \log p(y|\theta) + \log p(x|y,\theta)  
- \log p(y|x,\theta) \right] dy \\
&= \int_Y \frac{p(y|\hat\theta) p(x | y, \hat\theta)} { p(x| \hat\theta)}
 \left[ \log p(y|\theta) + \log p(x|y,\theta)  
- \log \frac{p(y|\theta) p(x | y, \theta)} { p(x| \theta)} \right] dy \\
&= \int_Y \frac{p(y|\hat\theta) p(x | y, \hat\theta)} { p(x| \hat\theta)}
 \log p(x| \theta) dy \\
\ea

OR:
\ba
p(x|\theta) - \int_Y p(y|\hat\theta) p(x | y, \hat\theta)dy = 0
\ea
Fixpoint iteration:
\ba
p(x|\theta^{(i+1)}) = \int_Y p(y|\theta^{(i)}) p(x | y, \theta^{(i)})dy 
\ea
Maximum-Likelihood-Estimation (ESTIMATION STEP):
\ba
\theta^{(k+1)} 
&= \argmax_{\theta} \sum_{i=1}^m \log p(x_i | \theta) 
= \argmax_{\theta} \sum_{i=1}^m \log \int_Y p(y|\theta) p(x_i | y, \theta)dy  \\
\ea
Classification (MAXIMIZATION STEP):
\ba
y
\ea




now set $\hat\theta := \theta^{k}$ and $\theta := \theta^{k+1}$ and define 
\ba
\theta^{k+1} = \argmax_\phi Q(\theta^k, \phi) + H(\theta^k, \phi)
\ea
Therefore the iteration goes like this:
\ba
p(x|\theta^{k+1}) &= -\int_Y p(y|x,\theta^{k}) \left[ \log p(x,y|\theta^{k+1}) - \log p(y|x,\theta^{k+1})\right] dy 
\ea
\subsection{Gaussian Mixtures}

Assume that $p(x| \theta)$ can be expressed as a weighted sum of Normal Distributions
\ba
p(x|\theta) = \sum_{i=1}^k \omega_i \sN_{\mu_i, C_i}(x)
\ea
where $C_i$ is the $i$-th covariance matrix.
The Expectation Maximization Algorithm then calculates a (local) optimum a choice of parameters $mu$ and $C$.

\subsection{k-means Algorithm}
The k-means algorithm is a crude simplification to the Gaussian Mixtures. It is defined by assuming that $C_i = I$ for all $i$.





