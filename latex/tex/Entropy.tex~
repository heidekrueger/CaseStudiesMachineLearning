
Let $p(X)$ be the probability density for the distribution $P$ of random variable X. The \textbf{required disk space} of a character $c$ is defined as
\ba
\text I(X = c) = \text I(X = c | P) = -\log(p(X = c)).
\ea
The \textbf{statistical Entropy} of a random variable X is defined as the expected disk space per character: 
\ba
\text H(X) = \E{\text I(X)}.
\ea
Statistical entropy can be interpeted as a measure of how uniform the distribution $P$ of $X$ is. Especially if the probability space $\Omega$ is finite, this statistical entropy can be used as a measure of how useful the data distribution is for inference. This is due to the fact that H is maximal for $X$ being uniformly distributed:

Consider $\text H(X) = -\sum_{i=1}^N p(x_i) \ld p(x_i)$ being a function $\text h$ dependant on the discrete probabilities $p_i := p(x_i)$: $\text H(X) = \text h(p_1, \dots, p_N) = -\sum_{i=1}^N p_i \ld p_i$. We want to look for the location of the maximum of $\text h$ with the constraint that $\sum_{i=1}^N = 1$. Using Lagrange Multiplier $\lambda$ we take the derivitive with repect to $p_i$:
\ba
\frac {\partial} {\partial p_i}  \text h(p_1, \dots, p_N) = \frac {\partial} {\partial p_i} \left(-\sum_{i=1}^N p(x_i) \ld p(x_i) + \lambda (\sum_{i=1}^N p(x_i) - 1) \right)= - \ld p_i - 1 + \lambda
\ea
If we look for the stationary point $\frac {\partial  \text h} {\partial p_i} = 0$ for all $p_i$, we easily get that $p_i = 2^{\lambda - 1}$ for all $i = 1, \dots, N$. Since this is a constant expression and considering $\sum_{i=1}^N = 1$ we immediately get that $p_i = p = \frac 1 N$ and $\lambda = 1 - \ld N$. \hfill q.e.d.


If one wants to estimate the propability density $p$ of a unknowningly distributed random variable, the \textbf{Cross-Entropy} is the measure of choice:
\ba
\text{H}(X | P | Q) = \text{H}(X) + \text D(P|Q) = \E{- \log Q(X)} = \E{\text I(X|Q)}
\ea
where D is the \textbf{Kullback-Leibler-Divergence}
\ba
\text D(P|Q) := \E{\log \frac{P(X)}{Q(X)}}.
\ea
The expectation value E in this context is always defined as the integral
\ba
\text E(X) := \int_\R p(x) \, x \, \text{dx}.
\ea


\section{Expectation Maximization Algorithm}

\begin{itemize}
\item[x] observable
\item[y] hidden variable
\item[$\theta$] parameterset
\end{itemize}

\ba
p(x,y| \theta) 
&= p(x|\theta) \frac{p(x,y,\theta)}{p(x|\theta) p(\theta)} \\
\Leftrightarrow
p(x,y| \theta) 
&= p(x|\theta) p(y|x,\theta) \\
\Leftrightarrow 
\log p(x| \theta) 
&=  \log p(x,y|\theta) - \log p(y|x,\theta) \\
\ea
Marginalization:
\ba
-\int_Y p(y|x,\hat\theta) \log p(x|\theta) dy &= -\int_Y p(y|x,\hat\theta) \log p(x,y|\theta) dy -\left(-\int_Y p(y|x,\hat\theta) \log p(y|x,\theta) dy \right) \\
&\Leftrightarrow \\
-\log p(x|\theta) =: Q(\hat\theta, \theta) + H(\hat\theta,\theta) \\
\text{observable information } &= \text{ complete information + hidden information }
\ea
now set $\hat\theta := \theta^{k}$ and $\theta := \theta^{k+1}$ and define 
\ba
\theta^{k+1} = \argmax_\phi Q(\theta^k, \phi) + H(\theta^k, \phi)
\ea
Therefore the iteration goes like this:
\ba
p(x|\theta^{k+1}) &= -\int_Y p(y|x,\theta^{k}) \left[ \log p(x,y|\theta^{k+1}) - \log p(y|x,\theta^{k+1})\right] dy 
\ea
\subsection{Gaussian Mixtures}

Assume that $p(x| \theta)$ can be expressed as a weighted sum of Normal Distributions
\ba
p(x|\theta) = \sum_{i=1}^k \omega_i \sN_{\mu_i, C_i}(x)
\ea
where $C_i$ is the $i$-th covariance matrix.
The Expectation Maximization Algorithm then calculates a (local) optimum a choice of parameters $mu$ and $C$.

\subsection{k-means Algorithm}
The k-means algorithm is a crude simplification to the Gaussian Mixtures. It is defined by assuming that $C_i = I$ for all $i$.



