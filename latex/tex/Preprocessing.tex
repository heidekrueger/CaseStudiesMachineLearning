


Curse of Dimensionality: ?

Postulates:
1. Samples:		Representative Samples
2. Features: 	A classifier is as good as its features
3. Compactness: Features belonging to the same class occupy a compact area in the feature space, and the different classes are reasonably separable

-> Low intra-class-distance, high inter-class-distance

4. Decomposition: Complex patterns can be decomposed into smaller parts, their combined presence makes up the pattern
5. Sturcture:	
6. Similarity:	Two representations are similar if a proper distance measure is small for them



\section{Principal Component Analysis (PCA)}
\label{PCA}

The PCA stems from Linear Algebra. It is relevant to know that if I have got two different vector spaces U and W with basis $\{u\}_j \in \R^m$ and $\{w\}_i \in \R^n$, respectively, 
I there is a matrix $A$, such that
\ba 
w_i = \sum_j a_{i,j} v_j.
\ea 
If $m = n$, by writing a vector x in both bases, we get that 
\ba 
\sum_j x_j v_j = x = \sum_i \sum_j a_{i,j} y_i v_j
\ea 
with new coordinates $y_i$. So, we can say: old coordinates = $A$ new coordinates
\ba 
\text{new coordinates } = A^{-1} \text{ old coordinates. }
\ea 
If the old basis is in the standard coordinate system with basis vectors $e_i \in \R^n$ such that $e_{i,j} = \delta_{i,j}$, then the new basis $w_i$ is given as the rows of the matrix A.
Conversely, if I want to have a transform into the basis $w_i$, all I need to do, is to put the new basisvectors as rowvectors of the transformation matrix A.
If, furthermore, the new basis is orthonormal, then I can also write the basis vectors as columns of A and do the transformation by applying $A^{-1} = A^T$.

There is a theorem which says that to every real valued matrix $A \in \R^{m \times n}$ there exists a Singular Value Decomposition (SVD) with matrices $U \in \R^{m \times m}$, $V \in \R^{n \times n}$ and $\Sigma \in \R^{m \times n}$,
such that
\ba
A = U \Sigma V^T
\ea
The two matrices $U$ and $V$ are orthogonal matrices containing the eigenvectors as columns of $A$. Both, the old, and the new basis vectors.
Furthermore, the matrix $\Sigma = diag(\sigma_1, \dots, \sigma_m, 0, \dots 0)$ consists of the so called ``Singular Values'' on its diagonal. 

For a set of feature vectors $\{X_i\}_{i = 1}^M$, the SVD is now applied to the covariance matrix $C \in \R^{N \times N}$ with mean value of the training data $\bar \mu$.
\ba
C = \frac{1}{M-1} \sum\limits_{i=1}^M (X_i - \bar\mu)(X_i - \bar\mu)^T, \;\;\; \bar\mu = \frac{1}{M} \sum\limits_{i=1}^M X_i,
\ea 
Since the matrix $C$ is symmetric and real, according to the theorem of the ``Hauptachsentransfromation'', there exists a orthogonal matrix $U \in \R^{N \times N}$ such that
\ba
C = U D U^T \text{,  with diagnonal matrix } \Sigma \text{ containing the eigenvalues of } C.
\ea 
This means, that the covariance matrix is preserving the coordinate system. 
Moreover, the matrix $U^{-1} = U^T$ transformes any vector $ c \in \R^N $ into the basis of the eigenvectors.

If we use the SVD in order to compute the Hauptachsentransfromation of $C$, we get a basis of eigenvectors such that the eigenvalues are a set of ordered non-negative real numbers.
\ba
C &= U D U^T = \left(U D^{\half1} \right) \left(U D^{\half1} \right)^T \text{, and} \\
C^{-1} &= \left(U D^{\half1} \right)^{-T} \left(U D^{\half1} \right)^{-1} = \left(D^{-\half1} U^T \right)^T \left(D^{-\half1} U^T \right) 
\ea
In other words: By multyplying each sample $\{X_i \in \R^N\}_{i=1}^M $ with the matrix $B := D^{-\half1} U^T$, we get a normalized set of vectors with zero mean and uniform covariance.

\subsection{Derivation of Kernel PCA}
Assume the data set $\{X_i\}_{i=1}^M$ is of zero mean. 
Let $\phi: \R^N -> \R^H$ be a nonlinear lifting transform.

Thus, the covarianve matrix simplifies to $C = \frac{1}{M-1} \sum\limits_{k=1}^M X_k X_k^T$. 
Let $\{v_i\}_{i=1}^N$ be the set of eigenvectors of $C$ with corresponding eigenvalues $\{\lambda_i\}_{i=1}^N$.
For each i, we can write $v_i$ as a linear combination of samples:
\ba
v_i = \sum_{j=1}^M \alpha_{i,j} X_j.
\ea 
Now we rewrite the eigenvalue problem using this equality:
\ba
\sum_{k, j} \alpha_{i,j} X_k X_k^T X_j = (M-1) \lambda_i \sum_{j} \alpha_{i,j} X_j
\ea
Considering the sum of projections of all samples onto the eigenvector $v_i$, this equation plugs into
\ba
\sum_l^M X_l^T v_i 
= \sum_{l, k, j} X_l^T X_k X_k^T X_j \alpha_{i,j} 
= (M-1) \lambda_i \sum_{l, j} X_l^T X_j \alpha_{i,j} 
\ea
It is now time to introduce the \textbf{kernel matrix} $K$ with $K_{i,j} = k(X_i, X_j) = \phi(X_i)^T\phi(X_j)$ with kernel $k$ and feature transform $\phi$.
\ba
\sum_{l, j} \sum_k K_{l,k} K_{k,j} \alpha_{i,j} 
= \sum_{l, j} K_{l,j}^2 \alpha_{i,j} 
= (M-1) \lambda_i \sum_{l, j} K_{l,k} \alpha_{i,j} 
\ea
In matrix notation, we get the \textbf{kernel eigenvalue problem}
\ba
K^2 \alpha_i = (M-1) \lambda_i K \alpha_i 
\Leftrightarrow  K \left[ K \alpha_i - (M-1) \lambda_i \alpha_i \right] = 0
\Leftrightarrow  K \alpha_i = (M-1) \lambda_i \alpha_i
\ea
which tells us that all $\alpha_i$ can be obtained as an eigenvector of the kernel matrix $K$. Writing the transformed samples $\{X_k\}_k^M$ in the basis of the $v_i$, we can get the transformed values by means of
\ba
\phi(X) = \sum_i^N \phi(X)^T v_i = \sum_i^N \sum_j^M \alpha_{i,j} \phi(X)^T \phi(X_j)  
=  \sum_i^N \sum_j^M \alpha_{i,j} k(X, X_j) 
\ea
